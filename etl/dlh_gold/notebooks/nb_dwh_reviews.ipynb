{"cells": [{"cell_type": "markdown", "id": "aa2c03d8-cd3a-4f21-85e5-c0868f64abab", "metadata": {}, "source": "<div align=\"right\">\n  <img src=\"Resources/logo_unir.png\" alt=\"Logo UNIR\" height=\"150px\" width=\"25%\">\n</div>\n\n# **Universidad Internacional de La Rioja**\n## Escuela Superior de Ingenier\u00eda y Tecnolog\u00eda\n### M\u00e1ster Universitario en An\u00e1lisis y Visualizaci\u00f3n de Datos Masivos / Visual Analytics and Big Data\n\n### **Trabajo Final de M\u00e1ster**\n#### Presentado por:\n- Cepeda Ramos, Jefferson\n- Mosquera Arce, Samek Fernando "}, {"cell_type": "markdown", "id": "7f8ef539-58d3-4c8b-b883-c51fe37d1238", "metadata": {"tags": []}, "source": "---\n## **Objetivo del notebook: Proceso de ingesta de datos datasets rese\u00f1as en la capa Gold**\n\nEste proceso tiene como objetivo realizarla ingesta de los datos de rese\u00f1as de usuarios almacenados en la **capa Silver** en GCP (`gs://lk_silver/GSC/reviews/`) en la **capa Gold** en Google BigQuery, en el schema `dw_gold`.\n\nEl flujo general del proceso contempla los siguientes pasos:\n\n1.  **Lectura de los datos**:\n    * Leer el dataset de rese\u00f1as desde la capa Silver.\n\n2.  **Definir las tablas dimensionales**:\n    * Se definen las tablas dimensionales ingresando una llave subrrogada para los id de la relaci\u00f3n.\n\n3.  **Definir la tabla de hechos**:\n\n    * Se define una tabla de hechos con relaci\u00f3n a las tablas dimensionales creadas.\n\n5.  **Almacenamiento en Capa Gold**:\n    * Se hace uso del bucket temporal **dwn_gold** para almacenar los datos y posteriomente hacer la inserci\u00f3n sobre la base de datos en BigQuery.\n---"}, {"cell_type": "markdown", "id": "7bca0bd9-d17b-43f4-92c3-4e72260b53d9", "metadata": {"tags": []}, "source": "## Instanciar SparkSession"}, {"cell_type": "code", "execution_count": null, "id": "f42f3fe5-e916-4247-8ea8-f73b361429c6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "SparkSession inicializada.\n"}], "source": "# Inicializar SparkSession si no est\u00e1 ya disponible\ntry:\n    spark\nexcept NameError:\n    spark = SparkSession.builder.appName(\"AmazonReviewsProcessing\").getOrCreate()\n\nprint(\"SparkSession inicializada.\")"}, {"cell_type": "markdown", "id": "0bacd2c6-1df6-4e71-9424-1586ad762cd4", "metadata": {"tags": []}, "source": "## Insertar librer\u00edas"}, {"cell_type": "code", "execution_count": 18, "id": "a8e47ace-a644-4fa4-a6c3-c16a64c96d69", "metadata": {}, "outputs": [], "source": "# Importar las librer\u00edas necesarias\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, row_number, monotonically_increasing_id"}, {"cell_type": "markdown", "id": "f5323554-5407-494c-8650-6e5c4222fe3a", "metadata": {"tags": []}, "source": "## Definir Par\u00e1metros Base"}, {"cell_type": "code", "execution_count": 35, "id": "09e09199-f782-4191-9718-84ffbb2cb0b2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Par\u00e1metros definidos\n"}], "source": "# Definir el esquema expl\u00edcito para los datasets de metadatos (productos)\n# Rutas del datasets en GCP\n\n# Ruta origen del dataset en la capa Silver\nsilver_path = \"gs://lk_silver/GSC/reviews/\"\n\n# Nombre Schema en la capa Gold\ngold_database = \"proyecto-tfm-unir\"\ngold_schema = \"dw_gold\"\ngold_bucket = \"dwh_gold\"\n\nprint(\"Par\u00e1metros definidos\")"}, {"cell_type": "markdown", "id": "d1fdf686-e095-4fcf-8c6a-67f6d5a395b0", "metadata": {}, "source": "## Leer los dataframe"}, {"cell_type": "code", "execution_count": 9, "id": "a0244822-f644-4beb-9a53-39072cc9c5c8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Leyendo datos de: gs://lk_silver/GSC/reviews/\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 5:=======================================================> (32 + 1) / 33]\r"}, {"name": "stdout", "output_type": "stream", "text": "Total de registros leidos: 12383878\n\nMostrando el esquema final del DataFrame:\nroot\n |-- rating: float (nullable = true)\n |-- title: string (nullable = true)\n |-- asin: string (nullable = true)\n |-- parent_asin: string (nullable = true)\n |-- verified_purchase: boolean (nullable = true)\n |-- helpful_vote: long (nullable = true)\n |-- category: string (nullable = true)\n |-- datetime: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- review: string (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Leer el dataset desde la ruta en silver\nprint(f\"Leyendo datos de: {silver_path}\")\ndf = spark.read.parquet(silver_path)\n\ninitial_row_count = df.count()\n\nprint(f\"Total de registros leidos: {initial_row_count}\")\n\nprint(\"\\nMostrando el esquema final del DataFrame:\")\ndf.printSchema()"}, {"cell_type": "markdown", "id": "69329735-e8b0-479d-954a-ebce86e3db8b", "metadata": {}, "source": "## Definici\u00f3n de tablas dimensionales"}, {"cell_type": "code", "execution_count": 28, "id": "8a4e61d4-da18-4f70-bf39-fa8cd7c0adef", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nInicia el proceso de definici\u00f3n de las tablas dimensionales\n"}], "source": "print(\"\\nInicia el proceso de definici\u00f3n de las tablas dimensionales\")\n\n# dimensi\u00f3n categor\u00eda\ndim_category = df.select(\"category\").dropDuplicates()\ndim_category = dim_category.withColumn(\"category_id\", monotonically_increasing_id())\n\n# dimensi\u00f3n compras verificadas\ndim_verified_purchase = df.select(\"verified_purchase\").dropDuplicates()\ndim_verified_purchase = dim_verified_purchase.withColumn(\"verified_purchase_id\", monotonically_increasing_id())"}, {"cell_type": "code", "execution_count": 29, "id": "8f3f4451-4996-4bf1-8a29-adce92584b3f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+-----------+\n|           category|category_id|\n+-------------------+-----------+\n|        Video_Games|          0|\n|Musical_Instruments|          1|\n|           Software|          2|\n+-------------------+-----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 23:======================================================> (32 + 1) / 33]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------------+--------------------+\n|verified_purchase|verified_purchase_id|\n+-----------------+--------------------+\n|             true|                   0|\n|            false|                   1|\n+-----------------+--------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Verifica las dimensiones\ndim_category.show()\ndim_verified_purchase.show()"}, {"cell_type": "markdown", "id": "cc282ce4-1a36-4e90-beaa-b5f2918f7473", "metadata": {}, "source": "\n## Definici\u00f3n tabla de hechos"}, {"cell_type": "code", "execution_count": 34, "id": "9412c23d-b1e8-459f-b930-563a109ca542", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nInicia el proceso de definici\u00f3n de la tabla de hechos\n\nEsquema de la tabla de hechos:\nroot\n |-- user_id: string (nullable = true)\n |-- asin: string (nullable = true)\n |-- parent_asin: string (nullable = true)\n |-- datetime: string (nullable = true)\n |-- title: string (nullable = true)\n |-- review: string (nullable = true)\n |-- rating: float (nullable = true)\n |-- helpful_vote: long (nullable = true)\n |-- category_id: long (nullable = true)\n |-- verified_purchase_id: long (nullable = true)\n\n"}], "source": "print(\"\\nInicia el proceso de definici\u00f3n de la tabla de hechos\")\n# Define el df como fact\nfact = df\n\n# Agregar 'category_id' a la tabla fact\nfact = fact.join(\n    dim_category.select(\"category\", \"category_id\"),\n    on=\"category\",\n    how=\"left\"\n)\n\n# Agregar 'verified_purchase_id' a la tabla fact\nfact = fact.join(\n    dim_verified_purchase.select(\"verified_purchase\", \"verified_purchase_id\"),\n    on=\"verified_purchase\",\n    how=\"left\"\n)\n\n# Eliminar columnas no necesarias\nfact = fact.drop(\"category\", \"verified_purchase\")\n\n# Define el orden de las columnas para la ingesta en BigQuery\ncolumn_order = [\n    \"user_id\",\n    \"asin\",\n    \"parent_asin\",\n    \"datetime\",\n    \"title\",\n    \"review\",\n    \"rating\",\n    \"helpful_vote\",\n    \"category_id\",\n    \"verified_purchase_id\"\n]\n\n# Reordena las columnas\nfact = fact.select(*column_order)\n\nprint(\"\\nEsquema de la tabla de hechos:\")\nfact.printSchema()"}, {"cell_type": "markdown", "id": "0d9ec2b6-68df-4f69-a732-f86b6ef541db", "metadata": {"tags": []}, "source": "## Almacenar datos en capa Gold"}, {"cell_type": "code", "execution_count": 42, "id": "9259bb0c-e0af-485f-9b7f-1f9d7defe55f", "metadata": {}, "outputs": [], "source": "# Funci\u00f3n que hace el insert de los datos en BigQuery\ndef insert_df(df, table_name: str):\n\n    # Construimos el identificador completo de la tabla\n    table_id = f\"{gold_database}.{gold_schema}.{table_name}\"\n\n    # 2. Proceso de escritura en BigQuery\n    print(f\"\\nIniciando la ingesta de datos en la tabla: {table_id}\")\n    print(f\"Modo de escritura: overwrite\")\n    print(f\"Bucket temporal: {gold_bucket}\")\n\n    # Usamos el DataFrameWriter para configurar y ejecutar la operaci\u00f3n\n    (df.write\n      .format(\"bigquery\")\n      .option(\"table\", table_id)\n      .option(\"temporaryGcsBucket\", gold_bucket)\n      .mode(\"overwrite\")\n      .save()\n    )\n\n    print(f\"La tabla '{table_name}' ha sido creada/sobrescrita en el dataset '{dim_category}'.\")"}, {"cell_type": "code", "execution_count": 43, "id": "0485ff29-f0fb-4939-85f4-d21ea75165a3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nIniciando la ingesta de datos en la tabla: proyecto-tfm-unir.dw_gold.dim_category\nModo de escritura: overwrite\nBucket temporal: dwh_gold\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "La tabla 'dim_category' ha sido creada/sobrescrita en el dataset 'DataFrame[category: string, category_id: bigint]'.\n\nIniciando la ingesta de datos en la tabla: proyecto-tfm-unir.dw_gold.dim_verified_purchase\nModo de escritura: overwrite\nBucket temporal: dwh_gold\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "La tabla 'dim_verified_purchase' ha sido creada/sobrescrita en el dataset 'DataFrame[category: string, category_id: bigint]'.\n\nIniciando la ingesta de datos en la tabla: proyecto-tfm-unir.dw_gold.fact_reviews\nModo de escritura: overwrite\nBucket temporal: dwh_gold\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "La tabla 'fact_reviews' ha sido creada/sobrescrita en el dataset 'DataFrame[category: string, category_id: bigint]'.\n\n Proceso de insert de datos completado.\n"}], "source": "# Insert tablas dimensionales\ninsert_df(dim_category, \"dim_category\")\ninsert_df(dim_verified_purchase, \"dim_verified_purchase\")\n\n# Insert tabla hechos\ninsert_df(fact, \"fact_reviews\")\n\nprint(\"\\n Proceso de insert de datos completado.\")"}, {"cell_type": "markdown", "id": "4e1f81d7-7d44-4846-9601-331f4e4f2c8c", "metadata": {}, "source": "## Detener SparkSession"}, {"cell_type": "code", "execution_count": null, "id": "f97e3443-bb49-4897-b865-ca112c7f6065", "metadata": {}, "outputs": [], "source": "# Detener la SparkSession\nspark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}