{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2c03d8-cd3a-4f21-85e5-c0868f64abab",
   "metadata": {},
   "source": [
    "# **Universidad Internacional de La Rioja**\n",
    "## Escuela Superior de Ingeniería y Tecnología\n",
    "### Máster Universitario en Análisis y Visualización de Datos Masivos / Visual Analytics and Big Data\n",
    "\n",
    "### **Trabajo Final de Máster**\n",
    "#### Presentado por:\n",
    "- Cepeda Ramos, Jefferson\n",
    "- Mosquera Arce, Samek Fernando "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ef539-58d3-4c8b-b883-c51fe37d1238",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## **Objetivo del notebook: Captura y Almacenamiento de Datos desde Amazon Reviews 2023\n",
    "\n",
    "Este proceso tiene como objetivo capturar datos desde la fuente pública de **Amazon Reviews 2023** y almacenarlos en la **capa Bronze** del data lake (`bucket: lk_bronze`) en formato **Parquet**. La fuente proporciona los archivos en formato comprimido `.json.gz`, que serán procesados y transformados de manera controlada y estructurada.\n",
    "\n",
    "El flujo general del proceso contempla los siguientes pasos:\n",
    "\n",
    "1. **Insertar Librerías**  \n",
    "   Se instalan las dependencias necesarias para el consumo de datos, como `requests`, `pandas`, `pyarrow` y/o `polars`, según se requiera para la lectura, transformación y almacenamiento.\n",
    "\n",
    "2. **Definir Parámetros Base**  \n",
    "   Se configuran los parámetros principales, como la URL base del repositorio, las categorías a descargar y la ruta de almacenamiento temporal y final. Esta configuración permite modularidad y escalabilidad del proceso.\n",
    "\n",
    "3. **Consumir Datos vía HTTP**  \n",
    "   Se realiza una solicitud HTTP (`requests.get`) para descargar los archivos `.json.gz` correspondientes a cada categoría desde la URL del repositorio oficial de Amazon Reviews.\n",
    "\n",
    "4. **Procesar y Almacenar como Parquet**  \n",
    "   Una vez descargados, los archivos `.json.gz` se descomprimen y leen temporalmente para su transformación. Finalmente, se almacenan de forma independiente como archivos `.parquet` en la ruta:  `gs://lk_bronze/GSC/ ` +  `categoría `\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca0bd9-d17b-43f4-92c3-4e72260b53d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instanciar SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42f3fe5-e916-4247-8ea8-f73b361429c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession inicializada.\n"
     ]
    }
   ],
   "source": [
    "# Inicializar SparkSession si no está ya disponible\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.appName(\"AmazonReviewsProcessing\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession inicializada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacd2c6-1df6-4e71-9424-1586ad762cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Insertar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e47ace-a644-4fa4-a6c3-c16a64c96d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, LongType, ArrayType, MapType, BooleanType\n",
    "import os\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5323554-5407-494c-8650-6e5c4222fe3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definir Parámetros Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0327391-cbae-4e53-a82c-bccb94f03334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los enlaces a los datasets\n",
    "# Los datasets de reseñas (reviews)\n",
    "review_datasets = {\n",
    "    \"Software\": \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Software.jsonl.gz\",\n",
    "    \"Musical_Instruments\": \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Musical_Instruments.jsonl.gz\",\n",
    "    \"Video_Games\": \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Video_Games.jsonl.gz\"\n",
    "}\n",
    "\n",
    "# Los datasets de metadatos (meta)\n",
    "products_datasets = {\n",
    "    \"Software\": \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Software.jsonl.gz\",\n",
    "    \"Musical_Instruments\": \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Musical_Instruments.jsonl.gz\",\n",
    "    \"Video_Games\": \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Video_Games.jsonl.gz\"\n",
    "}\n",
    "\n",
    "data_type_products = \"products\"\n",
    "data_type_reviews = \"reviews\"\n",
    "\n",
    "# Definir la ruta de salida en el bucket de GCS\n",
    "gcs_output_path = \"gs://lk_bronze/GSC/\"\n",
    "\n",
    "# Definir el esquema explícito para los datasets de metadatos (productos)\n",
    "product_schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", FloatType(), True),\n",
    "    StructField(\"rating_number\", LongType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType()), True),\n",
    "    StructField(\"description\", ArrayType(StringType()), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StructType([\n",
    "        StructField(\"thumb\", StringType(), True),\n",
    "        StructField(\"large\", StringType(), True),\n",
    "        StructField(\"hi_res\", StringType(), True),\n",
    "        StructField(\"variant\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"videos\", ArrayType(StructType([\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"user_id\", StringType(), True) # Añadido 'user_id' según el ejemplo de JSON\n",
    "    ])), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"categories\", ArrayType(StringType()), True),\n",
    "    StructField(\"details\", StructType([\n",
    "        StructField(\"release_date\", StringType(), True), # Original: \"Release Date\"\n",
    "        StructField(\"date_first_listed_on_amazon\", StringType(), True), # Original: \"Date first listed on Amazon\"\n",
    "        StructField(\"developed_by\", StringType(), True), # Original: \"Developed By\"\n",
    "        StructField(\"size\", StringType(), True), # Original: \"Size\"\n",
    "        StructField(\"version\", StringType(), True), # Original: \"Version\"\n",
    "        StructField(\"application_permissions\", ArrayType(StringType()), True), # Original: \"Application Permissions\"\n",
    "        StructField(\"minimum_operating_system\", StringType(), True), # Original: \"Minimum Operating System\"\n",
    "        StructField(\"approximate_download_time\", StringType(), True) # Original: \"Approximate Download Time\"\n",
    "    ]), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"bought_together\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Definir el esquema para los datasets de reseñas (reviews)\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StructType([\n",
    "        StructField(\"small_image_url\", StringType(), True),\n",
    "        StructField(\"medium_image_url\", StringType(), True),\n",
    "        StructField(\"large_image_url\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    StructField(\"helpful_vote\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Inicializar cliente de Google Cloud Storage\n",
    "# Las credenciales se obtienen automáticamente en el entorno de Dataproc\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdff536-7116-4e5e-b05a-19db04eac3e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalizar archivos JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a04d7f6-f396-4901-96de-c3c33712ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para normalizar claves JSON, ajustada para el schema de products\n",
    "def normalize_json_keys_products(json_string):\n",
    "    \"\"\"\n",
    "    Parsea una cadena JSON, normaliza sus claves y construye un objeto Row que\n",
    "    coincide estrictamente con el 'product_schema' definido.\n",
    "    Maneja la normalización de claves en top-level y dentro de 'details',\n",
    "    y asegura la compatibilidad de tipos para la creación del DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        record = json.loads(json_string)\n",
    "        final_record_for_row = {}\n",
    "\n",
    "        # Iterar sobre los campos definidos en el product_schema\n",
    "        for field_in_schema in product_schema.fields:\n",
    "            field_name_in_schema = field_in_schema.name # Nombre del campo en el esquema (ej: 'main_category', 'details')\n",
    "            \n",
    "            # Valor a asignar al campo en el Row, por defecto None\n",
    "            value_for_row_field = None\n",
    "\n",
    "            # Normalizar las claves de top-level para buscar en el JSON original\n",
    "            # Convertir el nombre del campo del esquema a posibles formatos de clave en el JSON original\n",
    "            possible_json_keys = [field_name_in_schema] # snake_case\n",
    "            if '_' in field_name_in_schema:\n",
    "                possible_json_keys.append(field_name_in_schema.replace('_', ' ').title()) # Title Case with spaces\n",
    "                possible_json_keys.append(field_name_in_schema.replace('_', ' ').lower())  # lower case with spaces\n",
    "                # Add any other common variations if needed based on data\n",
    "            \n",
    "            # Buscar el valor en el nivel superior del registro JSON\n",
    "            for key_variant in possible_json_keys:\n",
    "                if key_variant in record:\n",
    "                    value_for_row_field = record[key_variant]\n",
    "                    break\n",
    "            \n",
    "            # Manejo especial para el campo 'details' (que es un StructType anidado)\n",
    "            if field_name_in_schema == \"details\":\n",
    "                processed_details = {}\n",
    "                if isinstance(value_for_row_field, dict):\n",
    "                    # Iterar sobre los sub-campos definidos en el esquema de 'details'\n",
    "                    for detail_sub_field in field_in_schema.dataType.fields:\n",
    "                        sub_field_name_in_schema = detail_sub_field.name # ej: 'release_date'\n",
    "\n",
    "                        # Normalizar las claves de los sub-campos para buscar en el diccionario 'details'\n",
    "                        possible_detail_json_keys = [sub_field_name_in_schema]\n",
    "                        if '_' in sub_field_name_in_schema:\n",
    "                            possible_detail_json_keys.append(sub_field_name_in_schema.replace('_', ' ').title())\n",
    "                            possible_detail_json_keys.append(sub_field_name_in_schema.replace('_', ' ').lower())\n",
    "                        \n",
    "                        detail_sub_field_value = None\n",
    "                        for detail_key_variant in possible_detail_json_keys:\n",
    "                            if detail_key_variant in value_for_row_field:\n",
    "                                detail_sub_field_value = value_for_row_field[detail_key_variant]\n",
    "                                break\n",
    "                        \n",
    "                        # Manejo específico para 'application_permissions' (ArrayType) dentro de 'details'\n",
    "                        if sub_field_name_in_schema == \"application_permissions\" and detail_sub_field_value is not None:\n",
    "                            if isinstance(detail_sub_field_value, str):\n",
    "                                processed_details[sub_field_name_in_schema] = [detail_sub_field_value] # Wrap string in list\n",
    "                            elif isinstance(detail_sub_field_value, list):\n",
    "                                processed_details[sub_field_name_in_schema] = [str(item) for item in detail_sub_field_value if item is not None]\n",
    "                            else:\n",
    "                                processed_details[sub_field_name_in_schema] = None\n",
    "                        else:\n",
    "                            processed_details[sub_field_name_in_schema] = detail_sub_field_value\n",
    "                \n",
    "                value_for_row_field = processed_details\n",
    "            \n",
    "            # Manejo para campos que son ArrayType (listas) en el esquema\n",
    "            elif isinstance(field_in_schema.dataType, ArrayType):\n",
    "                if value_for_row_field is not None:\n",
    "                    if isinstance(value_for_row_field, str): # Si es una cadena, conviértela a una lista de cadenas\n",
    "                        value_for_row_field = [value_for_row_field]\n",
    "                    elif not isinstance(value_for_row_field, list): # Si no es una lista ni una cadena, ponlo como None\n",
    "                        value_for_row_field = None\n",
    "                    else: # Si es una lista, asegúrate de que los elementos sean del tipo esperado (ej: StringType)\n",
    "                        # Para listas de structs (images, videos), es necesario un procesamiento más profundo\n",
    "                        if isinstance(field_in_schema.dataType.elementType, StructType):\n",
    "                            processed_list = []\n",
    "                            for item in value_for_row_field:\n",
    "                                if isinstance(item, dict):\n",
    "                                    # Para 'images' o 'videos', asegurar que las claves son las que espera el esquema\n",
    "                                    item_processed = {}\n",
    "                                    for sub_struct_field in field_in_schema.dataType.elementType.fields:\n",
    "                                        sub_struct_field_name = sub_struct_field.name\n",
    "                                        item_processed[sub_struct_field_name] = item.get(sub_struct_field_name)\n",
    "                                    processed_list.append(item_processed)\n",
    "                                else:\n",
    "                                    processed_list.append(None) # O un diccionario vacío si aplica\n",
    "                            value_for_row_field = processed_list\n",
    "                        else: # Para listas de tipos simples (StringType)\n",
    "                            value_for_row_field = [str(item) for item in value_for_row_field if item is not None]\n",
    "                else: # Si value_for_row_field es None\n",
    "                    value_for_row_field = [] # Default to empty list if nullable and not present\n",
    "\n",
    "            # Manejo para price (StringType en esquema, puede ser float en JSON)\n",
    "            elif field_name_in_schema == \"price\" and value_for_row_field is not None:\n",
    "                value_for_row_field = str(value_for_row_field) # Convertir a string para el esquema\n",
    "\n",
    "            # Asignar el valor final al registro que se convertirá en Row\n",
    "            final_record_for_row[field_name_in_schema] = value_for_row_field\n",
    "\n",
    "        # Crear el objeto Row con los datos que coinciden exactamente con el esquema\n",
    "        return Row(**final_record_for_row)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Advertencia: Línea JSON malformada detectada y omitida: {json_string[:100]}... Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado al normalizar claves: {e} para línea: {json_string[:100]}...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15d27f4d-83ae-4d33-8b94-9ed086eca53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para normalizar claves JSON, ajustada para el schema de reviews\n",
    "def normalize_json_keys_reviews(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        \n",
    "        # Extraer y castear explícitamente cada campo al tipo esperado por el esquema.\n",
    "        # Es CRÍTICO que los nombres de las claves del .get() (ej: 'overall', 'summary',\n",
    "        # 'reviewText', 'image', 'reviewerID', 'unixReviewTime', 'verified', 'vote')\n",
    "        # coincidan con las claves reales en los archivos .jsonl.gz.\n",
    "        # Si un campo no se encuentra en `data`, .get() devuelve None.\n",
    "\n",
    "        # 1. rating (FloatType) - la clave común en los datos crudos es 'overall'\n",
    "        rating_val = float(data.get(\"overall\")) if data.get(\"overall\") is not None else None\n",
    "\n",
    "        # 2. title (StringType) - la clave común en los datos crudos es 'summary'\n",
    "        title_val = str(data.get(\"summary\")) if data.get(\"summary\") is not None else None\n",
    "\n",
    "        # 3. text (StringType) - la clave común en los datos crudos es 'reviewText'\n",
    "        text_val = str(data.get(\"reviewText\")) if data.get(\"reviewText\") is not None else None\n",
    "\n",
    "        # 4. images (ArrayType(StructType)) - la clave común en los datos crudos es 'image'\n",
    "        formatted_images = []\n",
    "        raw_images = data.get(\"image\")\n",
    "        if raw_images:\n",
    "            if isinstance(raw_images, list):\n",
    "                for img_url in raw_images:\n",
    "                    if isinstance(img_url, str):\n",
    "                        formatted_images.append(Row(small_image_url=img_url, medium_image_url=img_url, large_image_url=img_url))\n",
    "            elif isinstance(raw_images, str): # Si es una sola URL como cadena\n",
    "                formatted_images.append(Row(small_image_url=raw_images, medium_image_url=raw_images, large_image_url=raw_images))\n",
    "        images_val = formatted_images if formatted_images else None\n",
    "\n",
    "        # 5. asin (StringType)\n",
    "        asin_val = str(data.get(\"asin\")) if data.get(\"asin\") is not None else None\n",
    "\n",
    "        # 6. parent_asin (StringType)\n",
    "        parent_asin_val = str(data.get(\"parent_asin\")) if data.get(\"parent_asin\") is not None else None\n",
    "\n",
    "        # 7. user_id (StringType) - la clave común en los datos crudos es 'reviewerID'\n",
    "        user_id_val = str(data.get(\"reviewerID\")) if data.get(\"reviewerID\") is not None else None\n",
    "\n",
    "        # 8. timestamp (LongType) - la clave común en los datos crudos es 'unixReviewTime'\n",
    "        timestamp_val = int(data.get(\"unixReviewTime\")) if data.get(\"unixReviewTime\") is not None else None\n",
    "\n",
    "        # 9. verified_purchase (BooleanType) - la clave común en los datos crudos es 'verified'\n",
    "        verified_purchase_val = False # Valor por defecto si no está presente o es nulo\n",
    "        raw_verified = data.get(\"verified\")\n",
    "        if raw_verified is not None:\n",
    "            if isinstance(raw_verified, bool):\n",
    "                verified_purchase_val = raw_verified\n",
    "            elif isinstance(raw_verified, str):\n",
    "                verified_purchase_val = (raw_verified.lower() == 'true')\n",
    "\n",
    "        # 10. helpful_vote (LongType) - la clave común en los datos crudos es 'vote'\n",
    "        helpful_vote_val = int(data.get(\"vote\")) if data.get(\"vote\") is not None else None\n",
    "\n",
    "        # Crear un objeto Row con EXACTAMENTE 10 campos, en el orden preciso de reviews_schema.\n",
    "        # El número de argumentos aquí DEBE coincidir con el número de campos en reviews_schema.\n",
    "        return Row(\n",
    "            rating=rating_val,\n",
    "            title=title_val,\n",
    "            text=text_val,\n",
    "            images=images_val,\n",
    "            asin=asin_val,\n",
    "            parent_asin=parent_asin_val,\n",
    "            user_id=user_id_val,\n",
    "            timestamp=timestamp_val,\n",
    "            verified_purchase=verified_purchase_val,\n",
    "            helpful_vote=helpful_vote_val\n",
    "        )\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Saltando línea JSON mal formada: {json_string[:100]}... Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Captura cualquier otro error inesperado durante el procesamiento de una línea\n",
    "        print(f\"Saltando línea debido a error inesperado: {json_string[:100]}... Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ba1a1-8dd6-415d-b09a-c30e9ad4b7df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Procesar consumo vía HTTP y Almacenar como Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e663f1-edb2-4d81-8734-c1cbb0c3f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_dataset(name, url, data_type, schema=None):\n",
    "    print(f\"\\nProcesando {data_type} para la categoría: {name} desde {url}\")\n",
    "    \n",
    "    # Extraer el nombre del bucket de la ruta gcs_output_path\n",
    "    # gcs_output_path es como gs://nombre_bucket/ruta/\n",
    "    bucket_name = gcs_output_path.split(\"//\")[1].split(\"/\")[0]\n",
    "    \n",
    "    # Crear una ruta temporal en GCS para el archivo .gz descargado\n",
    "    # Ejemplo: lk_bronze/temp_downloads/software_products.jsonl.gz\n",
    "    temp_gcs_filename = f\"temp_downloads/{name.lower()}_{data_type}.jsonl.gz\"\n",
    "    temp_gcs_path_full = f\"gs://{bucket_name}/{temp_gcs_filename}\"\n",
    "\n",
    "    blob = None # Inicializar blob fuera del try para que sea accesible en finally\n",
    "    try:\n",
    "        # 1. Descargar el archivo .gz de la URL\n",
    "        print(f\"Descargando {url} a una ubicación temporal en GCS...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status() # Lanza un HTTPError para respuestas de error (4xx o 5xx)\n",
    "\n",
    "        # 2. Subir el contenido descargado directamente a GCS\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(temp_gcs_filename)\n",
    "        \n",
    "        # Abrir el blob en modo escritura binaria y escribir los chunks directamente\n",
    "        with blob.open(\"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Archivo descargado y subido a GCS temporalmente: {temp_gcs_path_full}\")\n",
    "\n",
    "        # 3. Extraer la data del archivo .jsonl.gz desde la ubicación temporal en GCS\n",
    "        # Leer como texto y luego parsear para manejar duplicados de columnas\n",
    "        rdd = spark.sparkContext.textFile(temp_gcs_path_full)\n",
    "        \n",
    "        # Mapear el RDD de cadenas a un RDD de objetos Row con claves normalizadas\n",
    "        # Filtrar las líneas vacías y las que no se pudieron parsear\n",
    "        # La función normalize_json_keys ahora está diseñada para usar el product_schema global\n",
    "        if data_type == \"products\":\n",
    "            rows = rdd.filter(lambda x: x is not None and x.strip() != \"\").map(normalize_json_keys_products).filter(lambda x: x is not None)\n",
    "        else:\n",
    "            rows = rdd.filter(lambda x: x is not None and x.strip() != \"\").map(normalize_json_keys_reviews).filter(lambda x: x is not None)\n",
    "        \n",
    "        # Crear un DataFrame de Spark a partir del RDD de Rows, aplicando el esquema explícito\n",
    "        # El esquema se pasa como argumento, pero la función normalize_json_keys ya lo usa internamente    \n",
    "        df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "        print(f\"Número de registros leídos para {name} ({data_type}): {df.count()}\")\n",
    "\n",
    "        # Agregar columnas para identificar la categoría y el tipo de dato\n",
    "        df = df.withColumn(\"category\", lit(name))\n",
    "\n",
    "        # 4. Convertir la data a formato Parquet y guardarla en GCS\n",
    "        # La ruta final será gs://lk_bronze/GSC/<data_type>/<nombre_categoria>/\n",
    "        output_dir = os.path.join(f\"{gcs_output_path}{data_type}\", name.lower())\n",
    "        \n",
    "        # Usar modo 'overwrite' para reemplazar si ya existe, o 'append' para añadir\n",
    "        df.write.mode(\"overwrite\").parquet(output_dir)\n",
    "        print(f\"Datos de {name} ({data_type}) guardados exitosamente en Parquet en: {output_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {name} ({data_type}): {e}\")\n",
    "    finally:\n",
    "        # 5. Eliminar el archivo temporal de GCS\n",
    "        if blob: # Solo intentar eliminar si el blob fue creado\n",
    "            try:\n",
    "                blob.delete()\n",
    "                print(f\"Archivo temporal {temp_gcs_path_full} eliminado de GCS.\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Advertencia: No se pudo eliminar el archivo temporal {temp_gcs_path_full}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e80dc6-9ee2-41b9-be66-c2a263d1d646",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Iniciar procesamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7707a1-8d94-4c88-b711-aab1739a311d",
   "metadata": {},
   "source": [
    "### 1. Datasets products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6f97d1-8c70-434a-9164-88ba5b7430c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando products para la categoría: Software desde https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Software.jsonl.gz\n",
      "Descargando https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Software.jsonl.gz a una ubicación temporal en GCS...\n",
      "Archivo descargado y subido a GCS temporalmente: gs://lk_bronze/temp_downloads/software_products.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros leídos para Software (products): 89251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de Software (products) guardados exitosamente en Parquet en: gs://lk_bronze/GSC/products/software\n",
      "Archivo temporal gs://lk_bronze/temp_downloads/software_products.jsonl.gz eliminado de GCS.\n",
      "\n",
      "\n",
      "Procesando products para la categoría: Musical_Instruments desde https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Musical_Instruments.jsonl.gz\n",
      "Descargando https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Musical_Instruments.jsonl.gz a una ubicación temporal en GCS...\n",
      "Archivo descargado y subido a GCS temporalmente: gs://lk_bronze/temp_downloads/musical_instruments_products.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros leídos para Musical_Instruments (products): 213593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de Musical_Instruments (products) guardados exitosamente en Parquet en: gs://lk_bronze/GSC/products/musical_instruments\n",
      "Archivo temporal gs://lk_bronze/temp_downloads/musical_instruments_products.jsonl.gz eliminado de GCS.\n",
      "\n",
      "\n",
      "Procesando products para la categoría: Video_Games desde https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Video_Games.jsonl.gz\n",
      "Descargando https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Video_Games.jsonl.gz a una ubicación temporal en GCS...\n",
      "Archivo descargado y subido a GCS temporalmente: gs://lk_bronze/temp_downloads/video_games_products.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros leídos para Video_Games (products): 137269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de Video_Games (products) guardados exitosamente en Parquet en: gs://lk_bronze/GSC/products/video_games\n",
      "Archivo temporal gs://lk_bronze/temp_downloads/video_games_products.jsonl.gz eliminado de GCS.\n",
      "\n",
      "\n",
      "Proceso completado para todos los datasets de products meta.\n"
     ]
    }
   ],
   "source": [
    "# Procesar los datasets de metadatos products\n",
    "for name, url in products_datasets.items():\n",
    "    process_and_save_dataset(name, url, data_type_products, schema=product_schema)\n",
    "\n",
    "print(\"\\nProceso completado para todos los datasets de products meta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1081fc-a547-4c51-8eed-2c27d9e02aee",
   "metadata": {},
   "source": [
    "### 2. Datasets reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33859ffa-dd1e-44b7-8fd8-86932b1d10ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando reviews para la categoría: Software desde https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Software.jsonl.gz\n",
      "Descargando https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Software.jsonl.gz a una ubicación temporal en GCS...\n",
      "Archivo descargado y subido a GCS temporalmente: gs://lk_bronze/temp_downloads/software_reviews.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros leídos para Software (reviews): 4880181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de Software (reviews) guardados exitosamente en Parquet en: gs://lk_bronze/GSC/reviews/software\n",
      "Archivo temporal gs://lk_bronze/temp_downloads/software_reviews.jsonl.gz eliminado de GCS.\n",
      "\n",
      "\n",
      "Procesando reviews para la categoría: Musical_Instruments desde https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Musical_Instruments.jsonl.gz\n",
      "Descargando https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Musical_Instruments.jsonl.gz a una ubicación temporal en GCS...\n",
      "Archivo descargado y subido a GCS temporalmente: gs://lk_bronze/temp_downloads/musical_instruments_reviews.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros leídos para Musical_Instruments (reviews): 3017439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de Musical_Instruments (reviews) guardados exitosamente en Parquet en: gs://lk_bronze/GSC/reviews/musical_instruments\n",
      "Archivo temporal gs://lk_bronze/temp_downloads/musical_instruments_reviews.jsonl.gz eliminado de GCS.\n",
      "\n",
      "\n",
      "Procesando reviews para la categoría: Video_Games desde https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Video_Games.jsonl.gz\n",
      "Descargando https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Video_Games.jsonl.gz a una ubicación temporal en GCS...\n",
      "Archivo descargado y subido a GCS temporalmente: gs://lk_bronze/temp_downloads/video_games_reviews.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros leídos para Video_Games (reviews): 4624615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de Video_Games (reviews) guardados exitosamente en Parquet en: gs://lk_bronze/GSC/reviews/video_games\n",
      "Archivo temporal gs://lk_bronze/temp_downloads/video_games_reviews.jsonl.gz eliminado de GCS.\n",
      "\n",
      "\n",
      "Proceso completado para todos los datasets de reviews.\n"
     ]
    }
   ],
   "source": [
    "# Procesar los datasets de reviews\n",
    "for name, url in review_datasets.items():\n",
    "    process_and_save_dataset(name, url, data_type_reviews, schema=reviews_schema)\n",
    "\n",
    "print(\"\\nProceso completado para todos los datasets de reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f81d7-7d44-4846-9601-331f4e4f2c8c",
   "metadata": {},
   "source": [
    "## Detener SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97e3443-bb49-4897-b865-ca112c7f6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detener la SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
