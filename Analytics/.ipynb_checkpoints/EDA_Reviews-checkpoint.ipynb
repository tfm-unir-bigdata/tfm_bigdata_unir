{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67de40e4-bcbd-4634-a732-7fc87ca3a564",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "  <img src=\"./Resources/logo_unir.png\" alt=\"Logo UNIR\" height=\"150px\" width=\"25%\">\n",
    "  <p><b>Ing. Cepeda Ramos Jefferson @Autor</b></p>\n",
    "  <p><b>Ing. Mosquera Arce Samek Fernando @Autor</b></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956d228-f4eb-4cd3-b2fd-2959dcf166e5",
   "metadata": {},
   "source": [
    "## Instalar librerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5d87abf-b66e-4b74-95b7-9976d94a38d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (1.30.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: ujson in c:\\users\\samec\\.conda\\envs\\unir_tfm\\lib\\site-packages (5.10.0)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "     ---------------------------------------- 0.0/434.1 MB ? eta -:--:--\n",
      "      -------------------------------------- 9.4/434.1 MB 53.3 MB/s eta 0:00:08\n",
      "     - ------------------------------------ 20.2/434.1 MB 51.0 MB/s eta 0:00:09\n",
      "     -- ----------------------------------- 31.2/434.1 MB 50.7 MB/s eta 0:00:08\n",
      "     --- ---------------------------------- 42.2/434.1 MB 50.7 MB/s eta 0:00:08\n",
      "     ---- --------------------------------- 53.2/434.1 MB 51.3 MB/s eta 0:00:08\n",
      "     ----- -------------------------------- 64.0/434.1 MB 51.0 MB/s eta 0:00:08\n",
      "     ------ ------------------------------- 74.4/434.1 MB 51.1 MB/s eta 0:00:08\n",
      "     ------- ------------------------------ 84.7/434.1 MB 50.9 MB/s eta 0:00:07\n",
      "     -------- ----------------------------- 95.4/434.1 MB 50.8 MB/s eta 0:00:07\n",
      "     --------- --------------------------- 106.7/434.1 MB 51.2 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 117.4/434.1 MB 51.0 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 128.2/434.1 MB 50.8 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 139.2/434.1 MB 50.8 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 149.9/434.1 MB 50.9 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 161.2/434.1 MB 50.7 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 171.4/434.1 MB 50.9 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 182.5/434.1 MB 50.9 MB/s eta 0:00:05\n",
      "     ---------------- -------------------- 192.7/434.1 MB 50.9 MB/s eta 0:00:05\n",
      "     ----------------- ------------------- 203.2/434.1 MB 50.7 MB/s eta 0:00:05\n",
      "     ------------------ ------------------ 214.4/434.1 MB 51.0 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 225.7/434.1 MB 50.8 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 236.7/434.1 MB 50.9 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 248.0/434.1 MB 51.0 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 257.9/434.1 MB 50.9 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 269.0/434.1 MB 50.8 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 280.0/434.1 MB 50.8 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 291.0/434.1 MB 50.9 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 302.0/434.1 MB 50.9 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 312.7/434.1 MB 50.8 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 324.0/434.1 MB 50.8 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 335.0/434.1 MB 50.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 345.5/434.1 MB 50.9 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 357.0/434.1 MB 50.9 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 367.8/434.1 MB 50.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 377.7/434.1 MB 50.8 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 388.5/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 397.7/434.1 MB 50.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 409.7/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 420.7/434.1 MB 50.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  431.5/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 50.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 434.1/434.1 MB 44.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741273 sha256=a0250c527347f4e783ad7dca4a3c20742efd6c4fc137ca200f4cf203bc3325a1\n",
      "  Stored in directory: c:\\users\\samec\\appdata\\local\\pip\\cache\\wheels\\2d\\77\\9b\\12660be70f7f447940a0caede37ae208b2e0d1c8487dce52a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   ---------------------------------------- 2/2 [pyspark]\n",
      "\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install polars\n",
    "!pip install seaborn\n",
    "!pip install ujson\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141cde1-eece-47ee-9b2e-6f1244574fc2",
   "metadata": {},
   "source": [
    "## Importar librerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d919acca-57ef-4275-96fb-f3043debae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars\n",
    "import seaborn\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1344cbc-eedb-47fc-b223-4e22053c114b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Crear o recuperar una sesiÃ³n de Spark\u001b[39;00m\n\u001b[32m      4\u001b[39m spark = SparkSession.builder \\\n\u001b[32m      5\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mMiAplicaciÃ³nPySpark\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     .getOrCreate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\UNIR_TFM\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\UNIR_TFM\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         SparkContext(conf=conf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\UNIR_TFM\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\UNIR_TFM\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\UNIR_TFM\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear o recuperar una sesiÃ³n de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MiAplicaciÃ³nPySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c0722f-3be6-4740-9ee1-1bc77a48b416",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m lines = (line.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines())\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 3) creamos un RDD de strings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m rdd = spark.sparkContext.parallelize(lines)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 4) parseamos JSON y creamos un DataFrame\u001b[39;00m\n\u001b[32m     17\u001b[39m rows = rdd.map(json.loads).map(\u001b[38;5;28;01mlambda\u001b[39;00m d: Row(**d))\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "url = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Software.jsonl.gz\"\n",
    "\n",
    "# 1) bajamos el contenido comprimido en streaming\n",
    "r = requests.get(url, stream=True)\n",
    "r.raise_for_status()\n",
    "\n",
    "# 2) iteramos por lÃ­neas (gzip se descomprime al vuelo)\n",
    "lines = (line.decode(\"utf-8\") for line in r.iter_lines())\n",
    "\n",
    "# 3) creamos un RDD de strings\n",
    "rdd = spark.sparkContext.parallelize(lines)\n",
    "\n",
    "# 4) parseamos JSON y creamos un DataFrame\n",
    "rows = rdd.map(json.loads).map(lambda d: Row(**d))\n",
    "df_software = spark.createDataFrame(rows)\n",
    "\n",
    "df_software.printSchema()\n",
    "df_software.show(5, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
