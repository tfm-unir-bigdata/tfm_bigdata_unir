# Universidad Internacional de La Rioja  
## Escuela Superior de Ingenier√≠a y Tecnolog√≠a  
### M√°ster Universitario en An√°lisis y Visualizaci√≥n de Datos Masivos / Visual Analytics and Big Data  

---

## Trabajo final de m√°ÃÅster:  
**Procesado Masivo para el an√°lisis de rese√±as y productos de Amazon**  
**Trabajo fin de estudio presentado por:**  
- Cepeda Ramos, Jefferson  
- Mosquera Arce, Samek Fernando  

**Tipo de trabajo:** Desarrollo Software  
**Director:** Ambiente, Enrique de Miguel  

---

# tfm_bigdata_unir

---

## üìò Versi√≥n en Espa√±ol

### Descripci√≥n  
El proyecto implementa un pipeline ETL escalable para el procesamiento de grandes vol√∫menes de datos de rese√±as y productos de Amazon (McAuley, 2023), carg√°ndolos en un Data Lakehouse optimizado en Google Cloud Platform.

### Objetivo general  
Implementar un pipeline ETL para el procesamiento eficiente y escalable de grandes cantidades de datos, utilizando los conjuntos de datos de rese√±as y productos de Amazon como fuente, recolectados por McAuley (Rese√±as de Amazon ‚Äô23, 2023). Este pipeline llevar√° a cabo la extracci√≥n, transformaci√≥n, enriquecimiento y carga de datos en un Data Lakehouse optimizado para almacenamiento a gran escala y procesamiento anal√≠tico.

### Objetivos espec√≠ficos  
1. **Selecci√≥n de subconjuntos**  
   - ‚ÄúMusical_Instruments‚Äù: 3 017 438 rese√±as, 213 591 productos  
   - ‚ÄúVideogames‚Äù: 4 624 614 rese√±as, 137 268 productos  
   - ‚ÄúSoftware‚Äù: 4 880 180 rese√±as, 89 250 productos  
   (Total: 12 522 232 rese√±as, 440 109 productos, 12 962 321 registros).

2. **Arquitectura de datos**  
   Definici√≥n de componentes (almacenamiento, procesamiento, integraci√≥n), flujo de datos y tecnolog√≠as (BigQuery, Cloud Storage, Composer).

3. **An√°lisis exploratorio (EDA)**  
   Evaluaci√≥n de estructura, calidad, patrones y volumen de los datos seleccionados.

4. **Desarrollo del pipeline ETL**  
   Implementaci√≥n de un workflow gobernado y altamente escalable en GCP, con extracci√≥n, transformaci√≥n y enriquecimiento mediante Composer y Python.

5. **M√©tricas (KPIs)**  
   Definici√≥n y seguimiento de indicadores de rendimiento del pipeline: tiempo de procesamiento, coste en la nube, calidad de datos.

6. **Dashboard interactivo**  
   Visualizaci√≥n de KPIs y principales hallazgos en un panel accesible para usuarios de Business Analytics.

### Plataforma y tecnolog√≠as  
- **Cloud**: Google Cloud Platform (BigQuery, Cloud Storage, Composer)  
- **Lenguajes**: Python, SQL  
- **Orquestaci√≥n**: Airflow / Cloud Composer  
- **Visualizaci√≥n**: Power BI

### Estructura del repositorio  
- **Analytics**: Notebooks de EDA
- **Data**: Scripts de descarga y preprocesado
- **Dictionaries:** Diccionarios de pa√≠ses_ciudades y nombres_apellidos para la calidad de datos
- **Docs**: Documentaci√≥n adicional
	- **DashBoard**: Documentaci√≥n del dashboard
	- **data_models:** Documentaci√≥n de los modelos de datos (Data Marts) en BigQuery
	- **gcp_metrics:** Gr√°ficas con las m√©tricas de ejecuci√≥n del pipeline
- **etl**: C√≥digo del pipeline ETL
	- **lk_bronze:** Notebooks de la capa Bronze
	- **lk_silver:** Notebooks de la capa Silver
	- **dlh_gold:** Notebooks de la capa Gold
- **pipeline:** Archivo DAG y Python para Apache Airflow
- **README.md**: Descripci√≥n del proyecto

## Referencias
- Hou, Y., Li, J., He, Z., Yan, A., Chen, X. & McAuley, J. (2024). _Bridging Language and Items for Retrieval and Recommendation_. arXiv preprint arXiv:2403.03952.

---

## üìô English Version

### Description  
This project implements a scalable ETL pipeline to process large volumes of Amazon review and product data (McAuley, 2023), loading them into an optimized Data Lakehouse on Google Cloud Platform.

### General Objective  
Implement an ETL pipeline for the efficient and scalable processing of large amounts of data, using Amazon review and product datasets collected by McAuley (Amazon Reviews ‚Äô23, 2023). The pipeline will perform extraction, transformation, enrichment, and loading into a Data Lakehouse optimized for large-scale storage and analytical processing.

### Specific Objectives  
1. **Subset Selection**  
   - ‚ÄúMusical_Instruments‚Äù: 3,017,438 reviews, 213,591 products  
   - ‚ÄúVideogames‚Äù: 4,624,614 reviews, 137,268 products  
   - ‚ÄúSoftware‚Äù: 4,880,180 reviews, 89,250 products  
   (Total: 12,522,232 reviews, 440,109 products, 12,962,321 records).

2. **Data Architecture**  
   Define data architecture components (storage, processing, integration), data flow, and technologies (BigQuery, Cloud Storage, Composer).

3. **Exploratory Data Analysis (EDA)**  
   Assess data structure, quality, patterns, and volume for the selected subsets.

4. **ETL Pipeline Development**  
   Build a governed, highly scalable workflow on GCP using Composer and Python.

5. **Key Performance Indicators (KPIs)**  
   Define and monitor pipeline metrics: processing time, cloud cost, data quality.

6. **Interactive Dashboard**  
   Visualize KPIs and key insights in a dashboard for Business Analytics users.

### Platform & Technologies  
- **Cloud**: Google Cloud Platform (BigQuery, Cloud Storage, Composer)  
- **Languages**: Python, PySpark, sql
- **Orchestration**: Airflow / Cloud Composer  
- **Visualization**: Power BI

### Repository Structure
- **Analytics**: Notebooks de EDA
- **Data**: Download and preprocessing scripts
- **Dictionaries:** Country_city and country_name dictionaries for data quality
- **Docs**: Additional Documentation
	- **DashBoard**: Definition of the dashboard
	- **data_models:** Documentation models (Data Marts) in BigQuery
	- **gcp_metrics:** Pipeline execution metrics graphs
- **etl**: ETL pipeline code
	- **lk_bronze:** Notebooks Bronze layer
	- **lk_silver:** Notebooks Silver layer
	- **dlh_gold:** Notebooks Gold layer
- **pipeline:** DAG and Python file for Apache Airflow
- **README.md**: Project description

## References
- Hou, Y., Li, J., He, Z., Yan, A., Chen, X. & McAuley, J. (2024). _Bridging Language and Items for Retrieval and Recommendation_. arXiv preprint arXiv:2403.03952.
